{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9bf55b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing libraries\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import math\n",
    "from nltk.util import ngrams\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4839a848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read dataframes\n",
    "# df1 = pd.read_pickle('testing_data_static_2022-04-10.pkl')\n",
    "# df2 = pd.read_pickle('testing_data_update_2022-04-10_2023-04-10.pkl')\n",
    "# df3 = pd.read_pickle('testing_data_with_duplicates.pkl')\n",
    "\n",
    "# # Concatenate dataframes\n",
    "# testing_data = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "# # Remove rows with duplicate data in the 'raw' column\n",
    "# testing_data = testing_data.drop_duplicates(subset='raw')\n",
    "\n",
    "# # Reset index after dropping duplicates\n",
    "# testing_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "sm_df = pd.read_pickle('updated_testing_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ee64378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weekAuthored</th>\n",
       "      <th>textProcessed</th>\n",
       "      <th>count</th>\n",
       "      <th>tfIdfMatrix</th>\n",
       "      <th>values</th>\n",
       "      <th>symptoms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-02</td>\n",
       "      <td>this week's review of the covid-19 pandemic in...</td>\n",
       "      <td>39</td>\n",
       "      <td>{'week': 0.010084212298055277, ''s': 0.0134456...</td>\n",
       "      <td>[months, blood, week, days, guidelines, doctor...</td>\n",
       "      <td>{weakness, anxiety, decreased_sense_of_smell, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-09</td>\n",
       "      <td>officially part of team canada i didn't realiz...</td>\n",
       "      <td>247</td>\n",
       "      <td>{'attend': 0.0025816384077358536, 'music': 0.0...</td>\n",
       "      <td>[spread, learn, hepatitis, reduce, individuals...</td>\n",
       "      <td>{lightheadedness, sore_throat, rash, fatigue, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-16</td>\n",
       "      <td>hi everyone i just want to share something i w...</td>\n",
       "      <td>210</td>\n",
       "      <td>{'gleyber': 0.0091795922388769, 'torres': 0.00...</td>\n",
       "      <td>[tmzlive, trump loyalty, laps, flunked #, # tr...</td>\n",
       "      <td>{sore_throat, fatigue, anxiety, fever, hoarse_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-23</td>\n",
       "      <td>5 months later and saw a grand baby graduation...</td>\n",
       "      <td>222</td>\n",
       "      <td>{'emory': 0.021668235815188525, 'university': ...</td>\n",
       "      <td>[children, initially, initially admitted, body...</td>\n",
       "      <td>{sore_throat, rash, fatigue, anxiety, fever, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-30</td>\n",
       "      <td>student speaks on the ingraham angle on how sh...</td>\n",
       "      <td>212</td>\n",
       "      <td>{'#': 0.0410445380523044, 'staysafe': 0.003029...</td>\n",
       "      <td>[result, visitors, puerto rico, puerto, rico, ...</td>\n",
       "      <td>{sore_throat, rash, fatigue, seizures, anxiety...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-06-06</td>\n",
       "      <td>a new study warns that men with lower testoste...</td>\n",
       "      <td>213</td>\n",
       "      <td>{'@': 0.0839901855207587, 'kff': 0.00266710149...</td>\n",
       "      <td>[rep david, david clark, david, clark, memoria...</td>\n",
       "      <td>{sore_throat, fatigue, anxiety, fever, hoarse_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-06-13</td>\n",
       "      <td>i had the virus back in december 2019 i did no...</td>\n",
       "      <td>199</td>\n",
       "      <td>{'#': 0.0677963940765181, 'fortheloveofdata': ...</td>\n",
       "      <td>[cruise, passengers, prior, data, kids, childr...</td>\n",
       "      <td>{lightheadedness, sore_throat, rash, fatigue, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-06-20</td>\n",
       "      <td>for decades fda guidelines have limited gay an...</td>\n",
       "      <td>246</td>\n",
       "      <td>{'self-registration': 0.004104609625928548, 'p...</td>\n",
       "      <td>[tara, cruise, center, ship, delta, cruise shi...</td>\n",
       "      <td>{lightheadedness, sore_throat, rash, fatigue, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-06-27</td>\n",
       "      <td>has this been happening to anyone i had covid ...</td>\n",
       "      <td>205</td>\n",
       "      <td>{'info': 0.008259390675013488, ',': 0.00403204...</td>\n",
       "      <td>[children, players, kids helping, 3-year-old, ...</td>\n",
       "      <td>{sore_throat, rash, fatigue, seizures, anxiety...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-07-04</td>\n",
       "      <td>healthcare providers and pharmacies continue t...</td>\n",
       "      <td>209</td>\n",
       "      <td>{'learn': 0.007366218323538584, '#': 0.0598126...</td>\n",
       "      <td>[cruise, week, senate, senate hearing, county,...</td>\n",
       "      <td>{sore_throat, rash, fatigue, anxiety, fever, p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  weekAuthored                                      textProcessed  count  \\\n",
       "0   2021-05-02  this week's review of the covid-19 pandemic in...     39   \n",
       "1   2021-05-09  officially part of team canada i didn't realiz...    247   \n",
       "2   2021-05-16  hi everyone i just want to share something i w...    210   \n",
       "3   2021-05-23  5 months later and saw a grand baby graduation...    222   \n",
       "4   2021-05-30  student speaks on the ingraham angle on how sh...    212   \n",
       "5   2021-06-06  a new study warns that men with lower testoste...    213   \n",
       "6   2021-06-13  i had the virus back in december 2019 i did no...    199   \n",
       "7   2021-06-20  for decades fda guidelines have limited gay an...    246   \n",
       "8   2021-06-27  has this been happening to anyone i had covid ...    205   \n",
       "9   2021-07-04  healthcare providers and pharmacies continue t...    209   \n",
       "\n",
       "                                         tfIdfMatrix  \\\n",
       "0  {'week': 0.010084212298055277, ''s': 0.0134456...   \n",
       "1  {'attend': 0.0025816384077358536, 'music': 0.0...   \n",
       "2  {'gleyber': 0.0091795922388769, 'torres': 0.00...   \n",
       "3  {'emory': 0.021668235815188525, 'university': ...   \n",
       "4  {'#': 0.0410445380523044, 'staysafe': 0.003029...   \n",
       "5  {'@': 0.0839901855207587, 'kff': 0.00266710149...   \n",
       "6  {'#': 0.0677963940765181, 'fortheloveofdata': ...   \n",
       "7  {'self-registration': 0.004104609625928548, 'p...   \n",
       "8  {'info': 0.008259390675013488, ',': 0.00403204...   \n",
       "9  {'learn': 0.007366218323538584, '#': 0.0598126...   \n",
       "\n",
       "                                              values  \\\n",
       "0  [months, blood, week, days, guidelines, doctor...   \n",
       "1  [spread, learn, hepatitis, reduce, individuals...   \n",
       "2  [tmzlive, trump loyalty, laps, flunked #, # tr...   \n",
       "3  [children, initially, initially admitted, body...   \n",
       "4  [result, visitors, puerto rico, puerto, rico, ...   \n",
       "5  [rep david, david clark, david, clark, memoria...   \n",
       "6  [cruise, passengers, prior, data, kids, childr...   \n",
       "7  [tara, cruise, center, ship, delta, cruise shi...   \n",
       "8  [children, players, kids helping, 3-year-old, ...   \n",
       "9  [cruise, week, senate, senate hearing, county,...   \n",
       "\n",
       "                                            symptoms  \n",
       "0  {weakness, anxiety, decreased_sense_of_smell, ...  \n",
       "1  {lightheadedness, sore_throat, rash, fatigue, ...  \n",
       "2  {sore_throat, fatigue, anxiety, fever, hoarse_...  \n",
       "3  {sore_throat, rash, fatigue, anxiety, fever, p...  \n",
       "4  {sore_throat, rash, fatigue, seizures, anxiety...  \n",
       "5  {sore_throat, fatigue, anxiety, fever, hoarse_...  \n",
       "6  {lightheadedness, sore_throat, rash, fatigue, ...  \n",
       "7  {lightheadedness, sore_throat, rash, fatigue, ...  \n",
       "8  {sore_throat, rash, fatigue, seizures, anxiety...  \n",
       "9  {sore_throat, rash, fatigue, anxiety, fever, p...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define patterns for COVID-19 symptoms\n",
    "covid_symptoms = [\n",
    "    [{\"LOWER\": {\"IN\": [\"fever\", \"high temperature\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"headache\", \"migraine\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"cough\", \"coughing\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"shortness of breath\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"fatigue\", \"tiredness\", \"loss of energy\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"sore\", \"sore throat\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"congestion\", \"runny\", \"nose\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"muscle\", \"body\", \"aches\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"nausea\", \"vomiting\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"diarrhea\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"chills\", \"shivering\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"pressure on chest\", \"weight on my chest\", \"pressure in head\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"pink eye\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"rash\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"fainting\", \"dizziness\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"seizures\", \"seizure\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"confusion\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"abdominal pain\", \"stomach pain\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"loss of appetite\", \"not hungry\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"muscle\", \"joint\", \"joint pain\", \"muscle pain\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"difficulty sleeping\", \"insomnia\", \"can't sleep\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"feeling disoriented\", \"disoriented\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"numbness\", \"tingling\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"chest pain\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"swelling\", \"edema\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"bruising\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"loss of coordination\", \"uncoordinated\", \"poor balance\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"difficulty speaking\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"frequent urination\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"blood in urine\", \"hematuria\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"skin discoloration\", \"discoloration\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"decreased urination\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"swollen glands\", \"enlarged glands\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"hair loss\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"chapped lips\", \"chapped\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"puffy eyes\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"weight gain\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"hoarse voice\", \"hoarse\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"mood changes\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"cognitive issues\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"leg swelling\", \"swelling\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"hair thinning\", \"thinning\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"dry skin\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"weakness\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"tremors\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"depression\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"anxiety\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"irritability\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"insomnia\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"feeling cold\", \"feel cold\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"feeling hot\", \"feel hot\", \"sweats\", \"sweaty\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"difficulty breathing\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"chest tightness\", \"tightness in chest\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"palpitations\", \"heart palpitations\", \"fluttering\", \"racing heart\", \n",
    "                       \"heart fluttering\", \"irregular heartbeat\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"lightheadedness\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"dizziness\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"severe headache\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"stroke\", \"heart attack\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"vision loss\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"paralysis\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"aphasia\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"weakness in arms\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"weakness in legs\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"facial droop\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"slurred speech\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"difficulty swallowing\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"decreased sense of smell\", \"sense of smell\", \"smell\", \"loss of smell\"]}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"decreased sense of taste\", \"taste\", \n",
    "                       \"sense of taste\", \"loss of taste\", \"no taste\"]}}],\n",
    "]\n",
    "\n",
    "grouped_symptoms = {\n",
    "    \"fever\": [\"fever\", \"high temperature\"],\n",
    "    \"headache\": [\"headache\", \"migraine\"],\n",
    "    \"cough\": [\"cough\", \"coughing\"],\n",
    "    \"shortness_of_breath\": [\"shortness of breath\"],\n",
    "    \"fatigue\": [\"fatigue\", \"tiredness\", \"loss of energy\"],\n",
    "    \"sore_throat\": [\"sore\", \"sore throat\"],\n",
    "    \"congestion\": [\"congestion\", \"runny nose\"],\n",
    "    \"muscle_aches\": [\"muscle\", \"body aches\"],\n",
    "    \"nausea_vomiting\": [\"nausea\", \"vomiting\"],\n",
    "    \"diarrhea\": [\"diarrhea\"],\n",
    "    \"chills\": [\"chills\", \"shivering\"],\n",
    "    \"chest_head_pressure\": [\"pressure on chest\", \"weight on my chest\", \"pressure in head\"],\n",
    "    \"pink_eye\": [\"pink eye\"],\n",
    "    \"rash\": [\"rash\"],\n",
    "    \"dizziness\": [\"fainting\", \"dizziness\"],\n",
    "    \"seizures\": [\"seizures\", \"seizure\"],\n",
    "    \"confusion\": [\"confusion\"],\n",
    "    \"abdominal_pain\": [\"abdominal pain\", \"stomach pain\"],\n",
    "    \"loss_of_appetite\": [\"loss of appetite\", \"not hungry\"],\n",
    "    \"muscle_joint_pain\": [\"muscle\", \"joint pain\"],\n",
    "    \"difficulty_sleeping\": [\"difficulty sleeping\", \"insomnia\", \"can't sleep\"],\n",
    "    \"feeling_disoriented\": [\"feeling disoriented\", \"disoriented\"],\n",
    "    \"numbness_tingling\": [\"numbness\", \"tingling\"],\n",
    "    \"chest_pain\": [\"chest pain\"],\n",
    "    \"swelling_edema\": [\"swelling\", \"edema\"],\n",
    "    \"bruising\": [\"bruising\"],\n",
    "    \"loss_of_coordination\": [\"loss of coordination\", \"uncoordinated\", \"poor balance\"],\n",
    "    \"difficulty_speaking\": [\"difficulty speaking\"],\n",
    "    \"frequent_urination\": [\"frequent urination\"],\n",
    "    \"blood_in_urine\": [\"blood in urine\", \"hematuria\"],\n",
    "    \"skin_discoloration\": [\"skin discoloration\", \"discoloration\"],\n",
    "    \"decreased_urination\": [\"decreased urination\"],\n",
    "    \"swollen_glands\": [\"swollen glands\", \"enlarged glands\"],\n",
    "    \"hair_loss\": [\"hair loss\"],\n",
    "    \"chapped_lips\": [\"chapped lips\", \"chapped\"],\n",
    "    \"puffy_eyes\": [\"puffy eyes\"],\n",
    "    \"weight_gain\": [\"weight gain\"],\n",
    "    \"hoarse_voice\": [\"hoarse voice\", \"hoarse\"],\n",
    "    \"mood_changes\": [\"mood changes\"],\n",
    "    \"cognitive_issues\": [\"cognitive issues\"],\n",
    "    \"leg_swelling\": [\"leg swelling\"],\n",
    "    \"hair_thinning\": [\"hair thinning\", \"thinning\"],\n",
    "    \"dry_skin\": [\"dry skin\"],\n",
    "    \"weakness\": [\"weakness\"],\n",
    "    \"tremors\": [\"tremors\"],\n",
    "    \"depression\": [\"depression\"],\n",
    "    \"anxiety\": [\"anxiety\"],\n",
    "    \"irritability\": [\"irritability\"],\n",
    "    \"insomnia\": [\"insomnia\"],\n",
    "    \"feeling_cold\": [\"feeling cold\", \"feel cold\"],\n",
    "    \"feeling_hot\": [\"feeling hot\", \"feel hot\", \"sweats\", \"sweaty\"],\n",
    "    \"difficulty_breathing\": [\"difficulty breathing\"],\n",
    "    \"chest_tightness\": [\"chest tightness\", \"tightness in chest\"],\n",
    "    \"palpitations\": [\"palpitations\", \"heart palpitations\", \"fluttering\", \"racing heart\", \"heart fluttering\", \"irregular heartbeat\"],\n",
    "    \"lightheadedness\": [\"lightheadedness\"],\n",
    "    \"severe_headache\": [\"severe headache\"],\n",
    "    \"stroke_heart_attack\": [\"stroke\", \"heart attack\"],\n",
    "    \"vision_loss\": [\"vision loss\"],\n",
    "    \"paralysis\": [\"paralysis\"],\n",
    "    \"aphasia\": [\"aphasia\"],\n",
    "    \"weakness_in_arms\": [\"weakness in arms\"],\n",
    "    \"weakness_in_legs\": [\"weakness in legs\"],\n",
    "    \"facial_droop\": [\"facial droop\"],\n",
    "    \"slurred_speech\": [\"slurred speech\"],\n",
    "    \"difficulty_swallowing\": [\"difficulty swallowing\"],\n",
    "    \"decreased_sense_of_smell\": [\"decreased sense of smell\", \"sense of smell\", \"smell\", \"loss of smell\"],\n",
    "    \"decreased_sense_of_taste\": [\"decreased sense of taste\", \"taste\", \"sense of taste\", \"loss of taste\", \"no taste\"]\n",
    "}\n",
    "\n",
    "matcher.add(\"COVID_SYMPTOMS\", covid_symptoms)\n",
    "\n",
    "weekly_text = pd.read_pickle('weekly_tf_idf.pkl')\n",
    "# weekly_text['symptoms'] = None\n",
    "\n",
    "for index, row in weekly_text.iloc[::-1].iterrows():\n",
    "    if weekly_text.at[index, 'symptoms'] == None:\n",
    "        symptoms = set()\n",
    "        doc = nlp(row['textProcessed'])\n",
    "\n",
    "        matches = matcher(doc)\n",
    "\n",
    "        # Extract matched spans\n",
    "        for match_id, start, end in matches:\n",
    "            matched_span = doc[start:end]\n",
    "\n",
    "            # Check if the matched span text is in any grouped symptom set\n",
    "            for symptom_group, symptom_set in grouped_symptoms.items():\n",
    "                if matched_span.text.lower() in symptom_set:\n",
    "                    symptoms.add(symptom_group)\n",
    "                    break  # Stop searching for other groups once found\n",
    "\n",
    "        weekly_text.at[index, 'symptoms'] = symptoms\n",
    "\n",
    "weekly_text.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8123b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weekly_text = pd.read_pickle('weekly_tf_idf.pkl')\n",
    "weekly_text.head(10)\n",
    "weekly_text.to_pickle('weekly_tf_idf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8778dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentiment Analysis (VADER)\n",
    "# authoredAt column datetime manipulation for timeseries grouping\n",
    "sm_df['authoredAt'] = pd.to_datetime(sm_df['authoredAt'])\n",
    "sm_df['authoredAt'] = sm_df['authoredAt'].dt.date.astype('datetime64[ns]')\n",
    "sm_df['weekAuthored'] = sm_df['authoredAt'].dt.isocalendar().week\n",
    "\n",
    "platform_list = sm_df['platform'].unique()\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "sm_df['negative'] = None\n",
    "sm_df['positive'] = None\n",
    "sm_df['compound'] = None\n",
    "sm_df['sentiment'] = None\n",
    "\n",
    "# print(sm_df.head(10))\n",
    "\n",
    "index = len(sm_df) - 1\n",
    "while index >= 0:\n",
    "    timeNotValid = False\n",
    "    sentimentNotValid = False\n",
    "    \n",
    "    if pd.isnull(sm_df.at[index, 'weekAuthored']) or not isinstance(sm_df.at[index, 'authoredAt'], pd.Timestamp):\n",
    "        # Check if 'weekAuthored' is null or 'authoredAt' is not of datetime type\n",
    "        # If any of the conditions are true, update the values\n",
    "        sm_df.at[index, 'authoredAt'] = pd.to_datetime(sm_df.at[index, 'authoredAt'], errors='coerce')\n",
    "        sm_df.at[index, 'authoredAt'] = sm_df.at[index, 'authoredAt'].date().astype('datetime64[ns]')\n",
    "        timeNotValid = True\n",
    "    \n",
    "    if (sm_df.at[index, 'negative'] is None) or (sm_df.at[index, 'positive'] is None) \\\n",
    "       or (sm_df.at[index, 'neutral'] is None) or (sm_df.at[index, 'compound'] is None):\n",
    "        text = sm_df.at[index, 'content']\n",
    "        sm_df.at[index, 'sentiment'] = analyzer.polarity_scores(text)\n",
    "        sm_df.at[index, 'negative'] = sm_df.at[index, 'sentiment']['neg']\n",
    "        sm_df.at[index, 'positive'] = sm_df.at[index, 'sentiment']['pos']\n",
    "        sm_df.at[index, 'neutral'] = sm_df.at[index, 'sentiment']['neu']\n",
    "        sm_df.at[index, 'compound'] = sm_df.at[index, 'sentiment']['compound']\n",
    "        sentimentNotValid = True\n",
    "\n",
    "    if not timeNotValid and not sentimentNotValid:\n",
    "        break\n",
    "        \n",
    "    index -= 1\n",
    "    \n",
    "# One-Hot Encoding Account Labels\n",
    "unique_values = set(val for sublist in sm_df['labels'] for val in sublist)\n",
    "# print(unique_values)\n",
    "for value in unique_values:\n",
    "    sm_df[value] = sm_df['labels'].apply(lambda x: 1 if value in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a31c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = {\n",
    "    \"'ll\", \"'tis\", \"'twas\", \"'ve\", \"10\", \"39\", \"a\", \"a's\", \"able\", \"ableabout\", \"about\", \"above\", \"abroad\",\n",
    "    \"abst\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\",\n",
    "    \"adopted\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\",\n",
    "    \"ago\", \"ah\", \"ahead\", \"ai\", \"ain't\", \"aint\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\",\n",
    "    \"alongside\", \"already\", \"also\", \"although\", \"always\", \"am\", \"amid\", \"amidst\", \"among\", \"amongst\", \"amoungst\",\n",
    "    \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\",\n",
    "    \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\",\n",
    "    \"approximately\", \"aq\", \"ar\", \"are\", \"area\", \"areas\", \"aren\", \"aren't\", \"arent\", \"arise\", \"around\", \"arpa\",\n",
    "    \"as\", \"aside\", \"ask\", \"asked\", \"asking\", \"asks\", \"associated\", \"at\", \"au\", \"auth\", \"available\", \"aw\", \"away\",\n",
    "    \"awfully\", \"az\", \"b\", \"ba\", \"back\", \"backed\", \"backing\", \"backs\", \"backward\", \"backwards\", \"bb\", \"bd\", \"be\",\n",
    "    \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"began\", \"begin\",\n",
    "    \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"beings\", \"believe\", \"below\", \"beside\", \"besides\",\n",
    "    \"best\", \"better\", \"between\", \"beyond\", \"bf\", \"bg\", \"bh\", \"bi\", \"big\", \"bill\", \"billion\", \"biol\", \"bj\", \"bm\",\n",
    "    \"bn\", \"bo\", \"both\", \"bottom\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"but\", \"buy\", \"bv\", \"bw\", \"by\", \"bz\",\n",
    "    \"c\", \"c'mon\", \"c's\", \"ca\", \"call\", \"came\", \"can\", \"can't\", \"cannot\", \"cant\", \"caption\", \"case\", \"cases\",\n",
    "    \"cause\", \"causes\", \"cc\", \"cd\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"ck\", \"cl\", \"clear\",\n",
    "    \"clearly\", \"click\", \"cm\", \"cmon\", \"cn\", \"co\", \"co.\", \"com\", \"come\", \"comes\", \"computer\", \"con\", \"concerning\",\n",
    "    \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"copy\", \"corresponding\",\n",
    "    \"could\", \"could've\", \"couldn\", \"couldn't\", \"couldnt\", \"course\", \"cr\", \"cry\", \"cs\", \"cu\", \"currently\", \"cv\",\n",
    "    \"cx\", \"cy\", \"cz\", \"d\", \"dare\", \"daren't\", \"darent\", \"date\", \"de\", \"dear\", \"definitely\", \"describe\", \"described\",\n",
    "    \"despite\", \"detail\", \"did\", \"didn\", \"didn't\", \"didnt\", \"differ\", \"different\", \"differently\", \"directly\", \"dj\",\n",
    "    \"dk\", \"dm\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doesnt\", \"doing\", \"don\", \"don't\", \"done\", \"dont\", \"doubtful\",\n",
    "    \"down\", \"downed\", \"downing\", \"downs\", \"downwards\", \"due\", \"during\", \"dz\", \"e\", \"each\", \"early\", \"ec\", \"ed\",\n",
    "    \"edu\", \"ee\", \"effect\", \"eg\", \"eh\", \"eight\", \"eighty\", \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"end\",\n",
    "    \"ended\", \"ending\", \"ends\", \"enough\", \"entirely\", \"er\", \"es\", \"especially\", \"et\", \"et-al\", \"etc\", \"even\",\n",
    "    \"evenly\", \"ever\", \"evermore\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\",\n",
    "    \"example\", \"except\", \"f\", \"face\", \"faces\", \"fact\", \"facts\", \"fairly\", \"far\", \"farther\", \"felt\", \"few\", \"fewer\",\n",
    "    \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fifty\", \"fify\", \"fill\", \"find\", \"finds\", \"fire\", \"first\", \"five\", \"fix\", \"fj\",\n",
    "    \"fk\", \"fm\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"forever\", \"former\", \"formerly\", \"forth\", \"forty\",\n",
    "    \"forward\", \"found\", \"four\", \"fr\", \"free\", \"from\", \"front\", \"full\", \"fully\", \"further\", \"furthered\",\n",
    "    \"furthering\", \"furthermore\", \"furthers\", \"fx\", \"g\", \"ga\", \"gave\", \"gb\", \"gd\", \"ge\", \"general\", \"generally\",\n",
    "    \"get\", \"gets\", \"getting\", \"gf\", \"gg\", \"gh\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gl\", \"gm\", \"gmt\", \"gn\",\n",
    "    \"go\", \"goes\", \"going\", \"gone\", \"good\", \"goods\", \"got\", \"gotten\", \"gov\", \"gp\", \"gq\", \"gr\", \"great\", \"greater\",\n",
    "    \"greatest\", \"greetings\", \"group\", \"grouped\", \"grouping\", \"groups\", \"gs\", \"gt\", \"gu\", \"gw\", \"gy\", \"h\", \"had\",\n",
    "    \"hadn\", \"hadn't\", \"hadnt\", \"half\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasn't\", \"hasnt\", \"have\", \"haven\",\n",
    "    \"haven't\", \"havent\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"hed\", \"hell\", \"hello\", \"help\",     \"hence\", \"her\", \"here\", \"here's\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"hers\", \"herself\",\n",
    "    \"herse\", \"hes\", \"hi\", \"hid\", \"high\", \"higher\", \"highest\", \"him\", \"himself\", \"himse\", \"his\", \"hither\", \"hk\",\n",
    "    \"hm\", \"hn\", \"home\", \"homepage\", \"hopefully\", \"how\", \"how'd\", \"how'll\", \"how's\", \"howbeit\", \"however\", \"hr\",\n",
    "    \"ht\", \"htm\", \"html\", \"http\", \"hu\", \"hundred\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"i.e.\", \"id\", \"ie\", \"if\",\n",
    "    \"ignored\", \"ii\", \"il\", \"ill\", \"im\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\",\n",
    "    \"inc\", \"inc.\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"inside\",\n",
    "    \"insofar\", \"instead\", \"int\", \"interest\", \"interested\", \"interesting\", \"interests\", \"into\", \"invention\",\n",
    "    \"inward\", \"io\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"isnt\", \"it\", \"it'd\", \"it'll\", \"it's\", \"itd\", \"itll\",\n",
    "    \"its\", \"itself\", \"itse\", \"ive\", \"j\", \"je\", \"jm\", \"jo\", \"join\", \"jp\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\",\n",
    "    \"kept\", \"keys\", \"kg\", \"kh\", \"ki\", \"kind\", \"km\", \"kn\", \"knew\", \"know\", \"known\", \"knows\", \"kp\", \"kr\", \"kw\",\n",
    "    \"ky\", \"kz\", \"l\", \"la\", \"large\", \"largely\", \"last\", \"lately\", \"later\", \"latest\", \"latter\", \"latterly\", \"lb\",\n",
    "    \"lc\", \"least\", \"length\", \"less\", \"lest\", \"let\", \"let's\", \"lets\", \"li\", \"like\", \"liked\", \"likely\", \"likewise\",\n",
    "    \"line\", \"little\", \"lk\", \"ll\", \"long\", \"longer\", \"longest\", \"look\", \"looking\", \"looks\", \"low\", \"lower\", \"lr\",\n",
    "    \"ls\", \"lt\", \"ltd\", \"lu\", \"lv\", \"ly\", \"m\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"making\", \"man\", \"many\",\n",
    "    \"may\", \"maybe\", \"mayn't\", \"maynt\", \"mc\", \"md\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"member\",\n",
    "    \"members\", \"men\", \"merely\", \"mg\", \"mh\", \"microsoft\", \"might\", \"might've\", \"mightn\", \"mightn't\", \"mightnt\",\n",
    "    \"mil\", \"mill\", \"million\", \"mine\", \"minus\", \"miss\", \"mk\", \"ml\", \"mm\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\",\n",
    "    \"mostly\", \"move\", \"mp\", \"mq\", \"mr\", \"mrs\", \"ms\", \"msie\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"must've\",\n",
    "    \"mustn\", \"mustn't\", \"mustnt\", \"mv\", \"mw\", \"mx\", \"my\", \"myself\", \"myse\", \"mz\", \"n\", \"na\", \"name\", \"namely\",\n",
    "    \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needed\", \"needing\",\n",
    "    \"needn't\", \"neednt\", \"needs\", \"neither\", \"net\", \"netscape\", \"never\", \"neverf\", \"neverless\", \"nevertheless\",\n",
    "    \"new\", \"newer\", \"newest\", \"next\", \"nf\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nl\", \"no\", \"no-one\", \"nobody\", \"non\",\n",
    "    \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"notwithstanding\",\n",
    "    \"novel\", \"now\", \"nowhere\", \"np\", \"nr\", \"nu\", \"null\", \"number\", \"numbers\", \"nz\", \"o\", \"obtain\", \"obtained\",\n",
    "    \"obviously\", \"of\", \"off\", \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"older\", \"oldest\", \"om\", \"omitted\", \"on\", \"once\",\n",
    "    \"one\", \"one's\", \"ones\", \"only\", \"onto\", \"open\", \"opened\", \"opening\", \"opens\", \"opposite\", \"or\", \"ord\", \"order\",\n",
    "    \"ordered\", \"ordering\", \"orders\", \"org\", \"other\", \"others\", \"otherwise\", \"ought\", \"oughtn't\", \"oughtnt\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"owing\", \"own\", \"p\", \"pa\", \"page\", \"pages\", \"part\",\n",
    "    \"parted\", \"particular\", \"particularly\", \"parting\", \"parts\", \"past\", \"pe\", \"per\", \"perhaps\", \"pf\", \"pg\", \"ph\",\n",
    "    \"pk\", \"pl\", \"place\", \"placed\", \"places\", \"please\", \"plus\", \"pm\", \"pmid\", \"pn\", \"point\", \"pointed\", \"pointing\",\n",
    "    \"points\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pr\", \"predominantly\", \"present\",\n",
    "    \"presented\", \"presenting\", \"presents\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"problem\",\n",
    "    \"problems\", \"promptly\", \"proud\", \"provided\", \"provides\", \"pt\", \"put\", \"puts\", \"pw\", \"py\", \"q\", \"qa\", \"que\",\n",
    "    \"quickly\", \"quite\", \"qv\", \"r\", \"ran\", \"rather\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\",\n",
    "    \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"reserved\",\n",
    "    \"respectively\", \"resulted\", \"resulting\", \"results\", \"right\", \"ring\", \"ro\", \"room\", \"rooms\", \"round\", \"ru\",\n",
    "    \"run\", \"s\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sb\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\",\n",
    "    \"seconds\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"sees\", \"self\", \"selves\",\n",
    "    \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"seventy\", \"several\", \"sg\", \"sh\", \"shall\", \"shan't\",\n",
    "    \"shant\", \"she\", \"she'd\", \"she'll\", \"she's\", \"shed\", \"shell\", \"shes\", \"should\", \"should've\", \"shouldn\",\n",
    "    \"shouldn't\", \"shouldnt\", \"show\", \"showed\", \"showing\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"sides\",\n",
    "    \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"site\", \"six\", \"sixty\", \"sj\",\n",
    "    \"sk\", \"sl\", \"slightly\", \"sm\", \"small\", \"smaller\", \"smallest\", \"sn\", \"so\", \"some\", \"somebody\", \"someday\",\n",
    "    \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\",\n",
    "    \"sorry\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sr\", \"st\", \"state\", \"states\", \"still\", \"stop\",\n",
    "    \"strongly\", \"su\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\",\n",
    "    \"sv\", \"sy\", \"system\", \"sz\", \"t\", \"t's\", \"take\", \"taken\", \"taking\", \"tc\", \"td\", \"tell\", \"ten\", \"tends\", \"test\",\n",
    "    \"text\", \"tf\", \"tg\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"that's\", \"that've\", \"thatll\",\n",
    "    \"thats\", \"thatve\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"there'd\",\n",
    "    \"there'll\", \"there're\", \"there's\", \"there've\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\",\n",
    "    \"therell\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"thereve\", \"these\", \"they\", \"they'd\",\n",
    "    \"they'll\", \"they're\", \"they've\", \"theyd\", \"theyll\", \"theyre\", \"theyve\", \"thick\", \"thin\", \"thing\", \"things\",\n",
    "    \"think\", \"thinks\", \"third\", \"thirty\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\",\n",
    "    \"thought\", \"thoughts\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"til\", \"till\",\n",
    "    \"tip\", \"tis\", \"tj\", \"tk\", \"tm\", \"tn\", \"to\", \"today\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\",\n",
    "    \"tp\", \"tr\", \"tried\", \"tries\", \"trillion\", \"truly\", \"try\", \"trying\", \"ts\", \"tt\", \"turn\", \"turned\", \"turning\",\n",
    "    \"turns\", \"tv\", \"tw\", \"twas\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tz\", \"u\", \"ua\", \"ug\", \"uk\", \"um\", \"un\",\n",
    "    \"under\", \"underneath\", \"undoing\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"up\",\n",
    "    \"upon\", \"ups\", \"upwards\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\",\n",
    "    \"uucp\", \"uy\", \"uz\", \"v\", \"va\", \"value\", \"various\", \"vc\", \"ve\", \"versus\", \"very\", \"vg\", \"vi\", \"via\", \"viz\",\n",
    "    \"vn\", \"vol\", \"vols\", \"vs\", \"vu\", \"w\", \"want\", \"wanted\", \"wanting\", \"wants\", \"was\", \"wasn\", \"wasn't\", \"wasnt\",\n",
    "    \"way\", \"ways\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"web\", \"webpage\", \"website\", \"wed\", \"welcome\", \"well\",\n",
    "    \"wells\", \"went\", \"were\", \"weren\", \"weren't\", \"werent\", \"weve\", \"wf\", \"what\", \"what'd\", \"what'll\", \"what's\",\n",
    "    \"what've\", \"whatever\", \"whatll\", \"whats\", \"whatve\", \"when\", \"when'd\", \"when'll\", \"when's\", \"whence\",\n",
    "    \"whenever\", \"where\", \"where'd\", \"where'll\", \"where's\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\",\n",
    "    \"whereupon\", \"wherever\", \"whether\", \"which\", \"whichever\", \"while\", \"whilst\", \"whim\", \"whither\", \"who\",\n",
    "    \"who'd\", \"who'll\", \"who's\", \"whod\", \"whoever\", \"whole\", \"wholl\", \"whom\", \"whomever\", \"whos\", \"whose\", \"why\",\n",
    "    \"why'd\", \"why'll\", \"why's\", \"widely\", \"width\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"won\",\n",
    "    \"won't\", \"wonder\", \"wont\", \"words\", \"work\", \"worked\", \"working\", \"works\", \"world\", \"would\", \"would've\",\n",
    "    \"wouldn\", \"wouldn't\", \"wouldnt\", \"ws\", \"www\", \"x\", \"y\", \"ye\", \"year\", \"years\", \"yes\", \"yet\", \"you\", \"you'd\",\n",
    "    \"you'll\", \"you're\", \"you've\", \"youd\", \"youll\", \"young\", \"younger\", \"youngest\", \"your\", \"youre\", \"yours\",\n",
    "    \"yourself\", \"yourselves\", \"youve\", \"yt\", \"yu\", \"z\", \"za\", \"zero\", \"zm\", \"zr\",\n",
    "    'covid', 'coronavirus', 'corona', 'rona', 'covid-19', 'tested','testing','test','tests',\n",
    "    'symptoms','positive','negative','para','vaccine','vaccines','vaccinated','vaxxed','virus''tests',\n",
    "    'people','health','pandemic','virus','sars-cov-2','doctor','covid19','vaccination','vaccinations','rt @',\n",
    "    '-- --','',\"I'm\", r'\\u', '&', 'amp','https'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f61f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controller function to generate TF-IDF Matrix\n",
    "def generate_matrix(sentences, documents):\n",
    "    sentences = nltk.sent_tokenize(text) # NLTK function\n",
    "    total_documents = documents\n",
    "\n",
    "    freq_matrix = _create_frequency_matrix(sentences)\n",
    "    tf_matrix = _create_tf_matrix(freq_matrix)\n",
    "    documents_per_words = _create_documents_per_words(freq_matrix)\n",
    "    idf_matrix = _create_idf_matrix(freq_matrix, documents_per_words, total_documents)\n",
    "    tf_idf_matrix = _create_tf_idf_matrix(tf_matrix, idf_matrix)\n",
    "    \n",
    "    return tf_idf_matrix\n",
    "\n",
    "# Create word frequency matrix for documents\n",
    "def _create_frequency_matrix(sentences):\n",
    "    frequency_matrix = {}\n",
    "    ps = SnowballStemmer(\"english\")\n",
    "\n",
    "    for sent in sentences:\n",
    "        freq_table = {}\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        dictionary_tokenizer = MWETokenizer(words, separator=' ') \n",
    "        dictionary_based_token = dictionary_tokenizer.tokenize(words) \n",
    "\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if word in stopWords:\n",
    "                continue\n",
    "\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "                \n",
    "        # Adding bigrams as phrases\n",
    "        bigrams = list(nltk.bigrams(words))\n",
    "        for bigram in bigrams:\n",
    "            phrase = ' '.join(bigram)\n",
    "            phrase_words = phrase.split(' ')\n",
    "            if all(word not in stopWords for word in phrase_words):\n",
    "                if phrase in freq_table:\n",
    "                    freq_table[phrase] += 1\n",
    "                else:\n",
    "                    freq_table[phrase] = 1\n",
    "\n",
    "        frequency_matrix[sent[:15]] = freq_table\n",
    "\n",
    "    return frequency_matrix\n",
    "\n",
    "# Create TF (text frequency) matrix for documents\n",
    "def _create_tf_matrix(freq_matrix):\n",
    "    tf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        tf_table = {}\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, count in f_table.items():\n",
    "            tf_table[word] = count / count_words_in_sentence\n",
    "\n",
    "        tf_matrix[sent] = tf_table\n",
    "\n",
    "    return tf_matrix\n",
    "\n",
    "# Find number of documents per words\n",
    "def _create_documents_per_words(freq_matrix):\n",
    "    word_per_doc_table = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        for word, count in f_table.items():\n",
    "            if word in word_per_doc_table:\n",
    "                word_per_doc_table[word] += 1\n",
    "            else:\n",
    "                word_per_doc_table[word] = 1\n",
    "\n",
    "    return word_per_doc_table\n",
    "\n",
    "# Create IDF (inverse document frequency) matrix for documents\n",
    "def _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n",
    "    idf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        idf_table = {}\n",
    "\n",
    "        for word in f_table.keys():\n",
    "            if float(count_doc_per_words[word]) == 0 or total_documents == 0:\n",
    "                idf_table[word] = 0.0\n",
    "            else:\n",
    "                idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n",
    "\n",
    "        idf_matrix[sent] = idf_table\n",
    "\n",
    "    return idf_matrix\n",
    "\n",
    "# TF-IDF = TF * IDF matrices\n",
    "def _create_tf_idf_matrix(tf_matrix, idf_matrix):\n",
    "    tf_idf_matrix = {}\n",
    "\n",
    "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
    "\n",
    "        tf_idf_table = {}\n",
    "\n",
    "        for (word1, value1), (word2, value2) in zip(f_table1.items(),\n",
    "                                                    f_table2.items()):  # here, keys are the same in both the table\n",
    "            tf_idf_table[word1] = float(value1 * value2)\n",
    "\n",
    "        tf_idf_matrix[sent1] = tf_idf_table\n",
    "\n",
    "    return tf_idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d95112",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_df['actualText'] = ''\n",
    "\n",
    "for index, row in sm_df.iterrows():\n",
    "    if (row['actualText'] == ''):\n",
    "        if (row['text_translated']) != None:\n",
    "            sm_df.at[index, 'actualText'] = str(row['text_translated'])\n",
    "        else:\n",
    "            sm_df.at[index, 'actualText'] = str(row['content'])\n",
    "\n",
    "# Use weekly-text dataframe to generate TF-IDF matrices\n",
    "weekly_text = sm_df.groupby([pd.Grouper(key='authoredAt', freq='W')])['actualText'].agg(\n",
    "    text_combined=' '.join,  # Aggregating text as before\n",
    "    count='count'  # Adding count aggregation for number of posts\n",
    ").reset_index()\n",
    "\n",
    "weekly_text = weekly_text.rename(columns={'authoredAt': 'weekAuthored'})\n",
    "weekly_text = weekly_text.rename(columns={'text_combined': 'textProcessed'})\n",
    "\n",
    "weekly_text['tfIdfMatrix'] = [{} for _ in range(len(weekly_text))]\n",
    "\n",
    "text = weekly_text.at[0, 'textProcessed']\n",
    "count = weekly_text.at[0,'count']\n",
    "\n",
    "for index, row in weekly_text[::-1].iterrows():\n",
    "    text = weekly_text.at[index, 'textProcessed']\n",
    "    count = weekly_text.at[index,'count']\n",
    "    \n",
    "    if row['tfIdfMatrix'] == {}:\n",
    "        matrix = generate_matrix(text, count)\n",
    "        # print(matrix)\n",
    "        \n",
    "        for dictionary in matrix.values():\n",
    "            weekly_text.at[index, 'tfIdfMatrix'] = dictionary\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "weekly_text.to_pickle('weekly_tf_idf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9734f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_df.to_pickle('updated_testing_data.pkl')\n",
    "weekly_text.to_pickle('weekly_tf_idf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5ffe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_df = pd.read_pickle('weekly_tf_idf.pkl')\n",
    "weekly_df['values'] = [[] for _ in range(len(weekly_text))]\n",
    "more_stopwords = [\n",
    "    'covid',\n",
    "    'coronavirus',\n",
    "    'corona',\n",
    "    'rona',\n",
    "    'covid-19',\n",
    "    'tested',\n",
    "    'testing',\n",
    "    'test',\n",
    "    'tests',\n",
    "    'symptoms',\n",
    "    'positive',\n",
    "    'negative',\n",
    "    'para',\n",
    "    'vaccine',\n",
    "    'vaccines',\n",
    "    'vaccinated',\n",
    "    'vaxxed',\n",
    "    'virus'\n",
    "    'tests',\n",
    "    'people',\n",
    "    'health',\n",
    "    'pandemic',\n",
    "    'virus',\n",
    "    'sars-cov-2',\n",
    "    'doctor',\n",
    "    'covid19',\n",
    "    'vaccination',\n",
    "    'vaccinations',\n",
    "    'rt @',\n",
    "    '-- --',\n",
    "    '',\n",
    "    \"I'm\", \n",
    "    r'\\u', \n",
    "    '&', \n",
    "    'amp',\n",
    "    'https']\n",
    "    \n",
    "# filtered_list = list(set(sorted_dict).difference(stopwords))\n",
    "    \n",
    "weekly_text['values'] = None\n",
    "\n",
    "for index, row in weekly_df.iterrows():\n",
    "    # Remove one-character words or strange numbers..\n",
    "    items = row['tfIdfMatrix'].items()\n",
    "    matrix_modified = {}\n",
    "    for key, value in items:  \n",
    "        if not key.isdigit() and len(key) > 3:\n",
    "            # Remove stopwords\n",
    "            words = key.lower().split()\n",
    "            \n",
    "            if all(word not in more_stopwords for word in words):\n",
    "                matrix_modified[key] = value\n",
    "\n",
    "    weekly_df.at[index, 'tfIdfMatrix'] = matrix_modified\n",
    "\n",
    "    sorted_df = pd.DataFrame.from_dict(matrix_modified, orient='index', columns=['tfIdfValue'])\n",
    "    sorted_df = sorted_df.reset_index()\n",
    "    sorted_df = sorted_df.rename(columns={'index' : 'keyword'})\n",
    "    sorted_df = sorted_df.sort_values(by='tfIdfValue', ascending=False)\n",
    "    sorted_dict = sorted_df['keyword'].tolist()\n",
    "    \n",
    "    for item in more_stopwords:\n",
    "        if item in sorted_dict:\n",
    "            sorted_dict.remove(item)\n",
    "    \n",
    "    sorted_dict = [item for item in sorted_dict if item]\n",
    "    \n",
    "    sorted_dict = sorted_dict[:20] # keep only the first 20 values\n",
    "    # print(sorted_dict)\n",
    "    weekly_text.at[index, 'values'] = sorted_dict\n",
    "    \n",
    "weekly_text.to_pickle('weekly_tf_idf.pkl')\n",
    "\n",
    "weekly_text.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fe3151",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sm_df['author'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72f7fe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame(weekly_text['values'].tolist(), index=weekly_text['weekAuthored'])\n",
    "\n",
    "words_df = words_df.reset_index()\n",
    "words_df.to_pickle('keywords.pkl')\n",
    "\n",
    "words_df = pd.read_pickle('keywords.pkl')\n",
    "\n",
    "sm_df = pd.read_pickle('updated_testing_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6aa5b463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID-19 Long Haulers Support' 'Survivor Corps' 'Vaccines save lives'\n",
      " 'COVID-19 Novel Coronavirus FACTS' '¡MÉDICOS POR LA VERDAD!'\n",
      " 'Black News Network (BNN)'\n",
      " 'COVID19: Real Talk from Health Care Workers around the Globe'\n",
      " 'Black Educators' 'Covid Wellness Clinic'\n",
      " 'Coronavirus Updates for: Statesboro, Georgia & Surrounding Counties'\n",
      " 'Athens GA COVID-19 Resources and Discussion' 'Georgia Trump Republicans'\n",
      " \"Skip Mason's Vanishing Black Atlanta History\" 'DeKalb Strong'\n",
      " 'COVID-19 Watch North GA w/ Help & Resources'\n",
      " 'Albany, GA Area Happenings Over 21'\n",
      " 'Albany GA: Home Is Where The Heart Is'\n",
      " 'Type 1 Diabetes Recipes & Food Ideas'\n",
      " 'Kimono My House (Virtual House Concerts)'\n",
      " 'Dank Diabetes Memes Diabuddies' 'America First Tea Party'\n",
      " 'The Prayer Wall' 'TERMINÓ 🧑\\u200d🦽🧑\\u200d🦽🧑\\u200d🦽🧑\\u200d🦽'\n",
      " 'Coronavirus Updates from NBC News']\n"
     ]
    }
   ],
   "source": [
    "facebook_df = sm_df[(sm_df['platform'] == 'facebook') & \n",
    "                    sm_df['raw'].apply(lambda x: 'account' in x \n",
    "                   and x['account'].get('accountType') == 'facebook_group' \n",
    "                                       if isinstance(x, dict) else False)]\n",
    "\n",
    "print(facebook_df['author'].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
